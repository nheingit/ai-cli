# AI Question Assistant

This tool provides an AI-powered question answering service that can be accessed
 through the command line. It sends requests to localhost:11434/api/generate. This is where Ollama exposes an API for models running on your local machine. The one used in this example is zephyr, a mistral-7B finetune. 

You can find Ollama docs on zephyr [here](https://ollama.ai/library/zephyr). You need this to be running in the background for this project to work.

If you pull this project down, you can then run the following command to enable cli usage.
```sh
npm install --global ai
```
and then run the following command:

```sh
ai ask <question>
```

Replace `<question>` with your query and the program will return an answer
generated by our intelligent assistant.

For example, you can ask:

```sh
ai ask 'who is simon cowell'
```

And the output would be something like this:

```text
Simon Cowell is a British television producer and judge of The X Factor,
Britain's Got Talent, and American Idol. He has also produced several talent
shows around the world.
```

This program is built using React and Ink, and uses Meow for command line
parsing. To learn more about this tool, check out the source code in the
`app.js` file. The main component of the application is exported as
`MainComponent`. It checks whether the input command is `ask` and whether a
question has been provided, and renders an AI Question Stream component for
generating answers accordingly. If not, it defaults to rendering the `App`
component with the provided name (command) as an argument.

For more information about using Meow for command line parsing, refer to their
[documentation](https://github.com/klauscfhq/meow#readme).

Feel free to contribute to this project and improve the AI question answering
capabilities!

* footnote: This readme was generated by this CLI ai helper. Please ping me on twitter if you have any questions about the project :)
